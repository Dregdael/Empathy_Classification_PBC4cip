# -*- coding: utf-8 -*-
"""Testing_trained_PBC4cip.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-xZxO35vnyGnruCBDDtIKxIs6Sn87xqi

# **Testing Trained PBC4cip**

This document will explore the testing of individual utterances to dialogue prompts found in the EmpatheticConversations database using the PBC4cip classification algorithm. In order to test the utterances please see further into the document for the random prompt and input a response as you see feet. Remember to keep the following files in the environment:

1.   EmpatheticConversations.xlsx - to obtain the prompts
2.   Empathyabase.csv - to ensure the format of the utterance is correct
3.   trained_pbc4cip.sav - trained PBC4cip classifier on the Empathyabase.csv database.
"""

pip install paralleldots

pip install PBC4cip

#Pandas and numpy imports
import pandas as pd
import numpy as np
from pandas.io.parsers.readers import read_csv

#PBC4cip import
from PBC4cip import PBC4cip
import os
import argparse
import numpy as np
import pandas as pd

from tqdm import tqdm, trange
from PBC4cip import PBC4cip
from PBC4cip.core.Evaluation import obtainAUCMulticlass
from PBC4cip.core.Helpers import get_col_dist, get_idx_val

#Pickle import
import pickle

#Import paralleldots
import paralleldots
from paralleldots import taxonomy
from paralleldots import set_api_key, get_api_key

#Import extras

import requests
import json
import re
import random

df = pd.read_excel('EmpatheticConversations.xlsx')
df_prepared = pd.read_csv('Empathyabase.csv')
prompt_df = df[df['utterance_idx'] == 1]
prompt_df = prompt_df.reset_index()
len(prompt_df)

"""## Set up paralleldots licence

This license is the one used by the creators of this document, the availability of the API might be inconsistent as a result. If you want to make sure you have access, please sign up through paralleldots and use an available license.
"""

paralleldots.set_api_key('9x4Ya0ooRZDwypZZzsIXOaNywIM6szzkk6yGZMX8e2U')
print( "API Key: %s" % paralleldots.get_api_key() )

"""#**INTERACTION SECTION**

Please read the prompt given to you and give an appropriate input.
"""

number_of_prompt = random.randint(0,len(prompt_df))
prompt = prompt_df.iloc[number_of_prompt]
print("Your conversation partner says: ")
print(prompt['utterance'])

response = input()

print("You say: ")
print(response)

#Initializing values
df.columns
prompt_values = {}

for c in df.columns:
  prompt_values.update({c:prompt[c]})

prompt_values['utterance_idx'] = 2
prompt_values['speaker_idx'] = prompt_values['speaker_idx']+1
prompt_values['utterance'] = response
prompt_values['ut_len'] = len(response)
prompt_values['Talker'] = 2

"""# **Paralleldots**

This part of the document gets the data from paralleldots.


# WARNING: ERRORS MIGHT APPEAR DURING FETCHING OF DATA, CHECK OUTPUT.

## YOU MIGHT NEED TO RUN THESE STEPS UNTIL NO ERRORS ARE STATED TO HAVE BEEN FOUND WHILE CONTACTING PARALLELDOTS
"""

api_key  = get_api_key()

"""### **Sentiment analysis**"""

prompt_values['Sentiment'] = str(paralleldots.sentiment(response))

"""### **Emotion analysis**"""

#prompt_values['Emotion'] = paralleldots.emotion(response,'en')
prompt_values['Emotion'] = requests.post( "https://apis.paralleldots.com/v4/emotion", data= { "api_key": api_key, "text": response, "lang_code": 'en' } ).text
re_emo = re.sub(r"\"", "'", prompt_values['Emotion'])
prompt_values['Emotion'] = re_emo

if "error-details" in str(prompt_values['Emotion']):
  print("Error contacting paralleldots!!!!")
  print("Setting up dummy values for emotion... ")
  prompt_values['Emotion'] = "{'emotion':{'Happy': 0.000000, 'Fear': 0.000000, 'Sad': 0.000000, 'Bored': 0.000000, 'Angry': 0.000000, 'Excited': 0.000000}}"
  print("DO NOT TRUST CLASSIFICATION RESULT")
  print("RECOMMENDATION: RESTART CODE BLOCK")
  print()
  print()

"""### **Taxonomic analysis**"""

#prompt_values['Taxonomy'] = paralleldots.taxonomy(response)

tax = requests.post( "https://apis.paralleldots.com/v4/taxonomy", data= { "api_key": api_key, "text": response } ).text
re_tax = re.sub(r"\"", "'", tax)
prompt_values['Taxonomy'] = re_tax

if "error-details" in str(prompt_values['Taxonomy']):
  print("Error contacting paralleldots!!!!")
  print("Setting up dummy values for Taxonomy... ")
  prompt_values['Taxonomy'] = "{'taxonomy':[{'confidence_score': 0.00000, 'tag': 'IMPACT'}, {'confidence_score': 0.00000, 'tag': 'EDUCATION'}, {'confidence_score': 0.00000, 'tag': 'POLITICS'}]}"
  print("DO NOT TRUST CLASSIFICATION RESULT")
  print("RECOMMENDATION: RESTART CODE BLOCK")
  print()
  print()

"""### **Intent analysis**"""

prompt_values['Intent'] = str(paralleldots.intent(str(response)))

"""# Utterance processing

In this section, we process the utterance so that it can be fed to the classifier. In general, it is just the separation of the paralleldots data and setting up the EC features so that it is compatible with the database format used to train the classifier.
"""

#Sentiment separation

def sentiment_separator(s,n):
  s2 = s[s.find(":")+1+s.find('{'):s.find("}")]
  s3 = s2[s2.find('{')+1:]
  array = s3.split(", ")
  #print(array)
  return float(array[n][array[n].find(':')+1:])

prompt_values['Negative_score'] = sentiment_separator(prompt_values['Sentiment'],0)
prompt_values['Neutral_score'] = sentiment_separator(prompt_values['Sentiment'],1)
prompt_values['Positivity_score'] = sentiment_separator(prompt_values['Sentiment'],2)

#Context encoding

num_to_context = {0: 'afraid',
 1: 'angry',
 2: 'annoyed',
 3: 'anticipating',
 4: 'anxious',
 5: 'apprehensive',
 6: 'ashamed',
 7: 'caring',
 8: 'confident',
 9: 'content',
 10: 'devastated',
 11: 'disappointed',
 12: 'disgusted',
 13: 'embarrassed',
 14: 'excited',
 15: 'faithful',
 16: 'furious',
 17: 'grateful',
 18: 'guilty',
 19: 'hopeful',
 20: 'impressed',
 21: 'jealous',
 22: 'joyful',
 23: 'lonely',
 24: 'nostalgic',
 25: 'prepared',
 26: 'proud',
 27: 'sad',
 28: 'sentimental',
 29: 'surprised',
 30: 'terrified',
 31: 'trusting'}

context_dict = dict((v, k) for k, v in num_to_context.items())

prompt_values['context_encoded'] = context_dict[prompt_values['context']]

#Emotion separation

emo_str = str(prompt_values['Emotion'])
emo_str = emo_str[emo_str.find(':{')+3:emo_str.find('}}')]
emo_arr = emo_str.split(',')
emo_dic = {}
for x in emo_arr:
  val = x.split(":")
  val[0] = re.sub(r"\'", "", val[0])
  emo_dic.update({re.match(r'[A-Za-z]*',val[0])[0]:float(val[1])})

for emo in emo_dic:
  prompt_values[emo] = emo_dic[emo]

#Intent separation

intents = ['news','query','spam','marketing','feedback','complaint','suggestion','appreciation']

def get_intent(s):
  s = s[s.find("':")+4:]
  arr = s.split(',')
  arr
  if len(arr) > 5:
    s = ''+ arr[4][:arr[4].find('{')]+ arr[4][arr[4].find("re':")+4:]
    arr[4] = s
    k = arr[5]
    k = k[k.find(': {')+3:]
    arr[5] = k
    t = arr[7]
    t = t[:t.find("}}")]
    arr[7] = t
  else:
    arr[4] = arr[4][:arr[4].find("}}")]
    arr.append("'complaint': 0.0")
    arr.append("'suggestion': 0.0")
    arr.append("'appreciation': 0.0")
  for i in range(len(arr)):
    arr[i] = arr[i].replace("'", "")
    arr[i] = arr[i].replace(" ", "")
    val = arr[i].split(':')
    arr[i] = [val[0],float(val[1])]
  return arr

intent_array = get_intent(prompt_values['Intent'])
for x in intent_array:
  prompt_values[str(x[0])] = x[1]

#Taxonomy separation

unique_tax_array = ['IMPACT', 'EDUCATION', 'POLITICS', 'ENTERTAINMENT', 'TASTE', 'SPORTS', 'HEALTHYLIVING', 'GREEN', 'BUSINESS', 'WORLDPOST', 'TECH', 'SCIENCE', 'ARTS&CULTURE', 'CRIME', 'TRAVEL', 'RELIGION']

def obtain_tax(s):
  s = s[s.find("[")+1:]
  s = s.split(',')
  #print(s)
  for x in s:
    if 'tag' not in s:
      s.remove(x)
  for i in range(len(s)):
    s[i] = s[i][s[i].find(':')+2:s[i].find("}")-1]
  return s

tax_dict = {}

def obtain_tax_sc(s):
#s = df['Taxonomy'][0]
  s = s[s.find("[")+1:]
  s = s[s.find("{")+1:s.find("}]}")]
  s = s.split('},{')
  for i in range(len(s)):
    arr = s[i].split(',')
    new_arr = [arr[1][arr[1].find(":'")+2:len(arr[1])-1],float(arr[0][arr[0].find(':')+1:])]
    #print(new_arr)
    s[i] = new_arr
  return s

tax_array = obtain_tax_sc(prompt_values['Taxonomy'].strip())

for tax in tax_array:
  tax_dict.update({tax[0]:tax[1]})

tax_dict

for tax in unique_tax_array:
  if tax in tax_dict:
    prompt_values[tax] = tax_dict[tax]
  else:
    prompt_values[tax] = float(0)

prompt_values

#Getting rid of unnecessary features
prompt_values.pop('context')
prompt_values.pop('conv_id')
prompt_values.pop('utterance')
prompt_values.pop('prompt')
prompt_values.pop('Sentiment')
prompt_values.pop('Emotion')
prompt_values.pop('Intent')
prompt_values.pop('speaker_idx')
prompt_values.pop('Taxonomy')

prompt_values

"""## We carry out the preparation of the database with our new row"""

#df_prepared2 = read_csv('Empathyabase-1tst.csv')
df_prepared2 = df_prepared.copy()
df_prepared2 = df_prepared2.append(prompt_values,ignore_index=True)

test = pd.DataFrame(df_prepared2, index=[len(df_prepared2)-1])
test['Empathy'] = test['Empathy'].astype('string')
test["utterance_idx"] = test["utterance_idx"].astype('category')
test["Talker"] = test["Talker"].astype('category')
test["context_encoded"] = test["context_encoded"].astype('category')
#df_prepared2

"""# CLASSIFICATION

In this section, we carry out classification using the saved PBC4cip model
"""

X1_test = test.drop(columns=['Empathy'])
Y1_test = test.drop(columns=X1_test.columns)

# save the model to disk
filename = 'trained_pbc4cip.sav'

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))

y_pred = loaded_model.predict(X1_test)

print("The classifier predicts that your response presents an empathy level of: "+ str(y_pred[0]) +" on a scale from 1 to 5")